apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ include "k3s-ha.fullname" . }}
  labels:
    {{- include "k3s-ha.labels" . | nindent 4 }}
spec:
  serviceName: {{ include "k3s-ha.fullname" . }}-headless
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app.kubernetes.io/name: {{ include "k3s-ha.name" . }}
      app.kubernetes.io/instance: {{ .Release.Name }}
  template:
    metadata:
      labels:
        app.kubernetes.io/name: {{ include "k3s-ha.name" . }}
        app.kubernetes.io/instance: {{ .Release.Name }}
    spec:
      {{- if .Values.hostNetwork.enabled }}
      hostNetwork: true
      dnsPolicy: ClusterFirstWithHostNet
      {{- end }}

      {{- if and .Values.antiAffinity.enabled (not .Values.affinity) }}
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/name: {{ include "k3s-ha.name" . }}
                  app.kubernetes.io/instance: {{ .Release.Name }}
              topologyKey: kubernetes.io/hostname
      {{- end }}

      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}

      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}

      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}

      containers:
        - name: k3s
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}

          securityContext:
            privileged: {{ .Values.securityContext.privileged }}

          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: K3S_TOKEN
              value: {{ .Values.token.value | quote }}

          command:
            - /bin/sh
            - -c
            - |
              set -euo pipefail

              NAME="${POD_NAME:-${HOSTNAME:-$(hostname)}}"
              NAME="${NAME%%.*}"
              ORDINAL="${NAME##*-}"

              HEADLESS="{{ include "k3s-ha.fullname" . }}-headless"
              NS="{{ .Release.Namespace }}"
              JOIN_HOST="{{ include "k3s-ha.fullname" . }}-0.${HEADLESS}.${NS}.svc.cluster.local"
              API_PORT="{{ .Values.k3s.apiPort }}"
              JOIN_URL="https://${JOIN_HOST}:${API_PORT}"

              EXTRA_ARGS=""
              {{- range .Values.k3s.extraArgs }}
              EXTRA_ARGS="$EXTRA_ARGS {{ . }}"
              {{- end }}
              EXTRA_ARGS="$EXTRA_ARGS --https-listen-port=${API_PORT}"

              TLS_SANS=""
              {{- range .Values.k3s.tlsSANs }}
              TLS_SANS="$TLS_SANS --tls-san {{ . }}"
              {{- end }}

              if [ "$ORDINAL" = "0" ]; then
                echo "Starting k3s server-0 with --cluster-init"
                exec k3s server \
                  --cluster-init \
                  --token "$K3S_TOKEN" \
                  $TLS_SANS \
                  $EXTRA_ARGS
              else
                echo "Starting k3s server-$ORDINAL joining: $JOIN_URL"
                until KUBECONFIG=/dev/null k3s kubectl \
                  --server "$JOIN_URL" \
                  --insecure-skip-tls-verify=true \
                  get --raw=/healthz >/dev/null 2>&1; do
                  echo "Waiting for API at $JOIN_URL/healthz to become available..."
                  sleep 2
                done
                exec k3s server \
                  --server "$JOIN_URL" \
                  --token "$K3S_TOKEN" \
                  $TLS_SANS \
                  $EXTRA_ARGS
              fi

          ports:
            - name: https
              containerPort: {{ .Values.k3s.apiPort }}
              protocol: TCP

          {{- if .Values.probes.enabled }}
          livenessProbe:
            tcpSocket:
              port: https
            initialDelaySeconds: {{ .Values.probes.liveness.initialDelaySeconds }}
            periodSeconds: {{ .Values.probes.liveness.periodSeconds }}
            timeoutSeconds: {{ .Values.probes.liveness.timeoutSeconds }}
            failureThreshold: {{ .Values.probes.liveness.failureThreshold }}
            successThreshold: {{ .Values.probes.liveness.successThreshold }}

          readinessProbe:
            tcpSocket:
              port: https
            initialDelaySeconds: {{ .Values.probes.readiness.initialDelaySeconds }}
            periodSeconds: {{ .Values.probes.readiness.periodSeconds }}
            timeoutSeconds: {{ .Values.probes.readiness.timeoutSeconds }}
            failureThreshold: {{ .Values.probes.readiness.failureThreshold }}
            successThreshold: {{ .Values.probes.readiness.successThreshold }}

          {{- if .Values.probes.startup.enabled }}
          startupProbe:
            tcpSocket:
              port: https
            periodSeconds: {{ .Values.probes.startup.periodSeconds }}
            timeoutSeconds: {{ .Values.probes.startup.timeoutSeconds }}
            failureThreshold: {{ .Values.probes.startup.failureThreshold }}
          {{- end }}
          {{- end }}

          volumeMounts:
            - name: k3s-data
              mountPath: /var/lib/rancher/k3s

          {{- with .Values.resources }}
          resources:
            {{- toYaml . | nindent 12 }}
          {{- end }}

  {{- if .Values.persistence.enabled }}
  volumeClaimTemplates:
    - metadata:
        name: k3s-data
      spec:
        accessModes:
          {{- toYaml .Values.persistence.accessModes | nindent 10 }}
        resources:
          requests:
            storage: {{ .Values.persistence.size }}
        {{- if .Values.persistence.storageClassName }}
        storageClassName: {{ .Values.persistence.storageClassName | quote }}
        {{- end }}
  {{- end }}
